{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7a737c1-37b1-4896-a3e5-607444f06d81",
   "metadata": {},
   "source": [
    "# XGBoost Modeling\n",
    "## Continuous Response (view_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4becd9c8-98bc-44ff-9b4b-042e41d0cc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: statsmodels in /home/jupyter-cradduhj/.local/lib/python3.12/site-packages (0.14.5)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in /opt/tljh/user/lib/python3.12/site-packages (from statsmodels) (2.1.3)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /opt/tljh/user/lib/python3.12/site-packages (from statsmodels) (1.14.1)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /home/jupyter-cradduhj/.local/lib/python3.12/site-packages (from statsmodels) (2.3.3)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /home/jupyter-cradduhj/.local/lib/python3.12/site-packages (from statsmodels) (1.0.2)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/tljh/user/lib/python3.12/site-packages (from statsmodels) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/tljh/user/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/tljh/user/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/tljh/user/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/tljh/user/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: xgboost in /home/jupyter-cradduhj/.local/lib/python3.12/site-packages (3.1.2)\n",
      "Requirement already satisfied: numpy in /opt/tljh/user/lib/python3.12/site-packages (from xgboost) (2.1.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /home/jupyter-cradduhj/.local/lib/python3.12/site-packages (from xgboost) (2.28.9)\n",
      "Requirement already satisfied: scipy in /opt/tljh/user/lib/python3.12/site-packages (from xgboost) (1.14.1)\n"
     ]
    }
   ],
   "source": [
    "# import/install librares/packages\n",
    "!pip install statsmodels\n",
    "!pip install xgboost\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import max_error, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "\n",
    "# initialize scaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf9a401b-3f47-4b93-8e57-5a984d4153eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['acousticness', 'danceability', 'energy', 'instrumentalness',\n",
      "       'liveness', 'loudness', 'speechiness', 'tempo', 'valence', 'view_count',\n",
      "       'chart_year', 'type_Group', 'type_Person', 'country_CA', 'country_GB',\n",
      "       'country_Other', 'country_US', 'key_C', 'key_C_Sharp', 'key_D',\n",
      "       'key_D_Sharp', 'key_E', 'key_F', 'key_F_Sharp', 'key_G', 'key_G_Sharp',\n",
      "       'key_A', 'key_A_Sharp', 'key_B', 'duration_min', 'begin_year', 'pop',\n",
      "       'rock', 'hip-hop', 'r&b', 'country', 'jazz', 'electronic', 'reggae',\n",
      "       'metal', 'folk', 'lyrics_pos', 'lyrics_neg', 'lyrics_neu',\n",
      "       'lyrics_compound', 'title_pos', 'title_neg', 'title_neu',\n",
      "       'title_compound', 'lyrics_subjectivity', 'word_count', 'avg_word_len',\n",
      "       'unique_words', 'vocab_richness', 'Rap_Street_Slang_Topic',\n",
      "       'Romance_Relationships_Topic', 'Life_Nostalgia_Topic',\n",
      "       'Party_Dance_Sensuality_Topic', 'Love_Emotion_Sentiment_Topic',\n",
      "       'Loss_Struggle_Reflection_Topic', 'Energy_Vibes_Epic_Topic'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "      <th>view_count</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>vocab_richness</th>\n",
       "      <th>Rap_Street_Slang_Topic</th>\n",
       "      <th>Romance_Relationships_Topic</th>\n",
       "      <th>Life_Nostalgia_Topic</th>\n",
       "      <th>Party_Dance_Sensuality_Topic</th>\n",
       "      <th>Love_Emotion_Sentiment_Topic</th>\n",
       "      <th>Loss_Struggle_Reflection_Topic</th>\n",
       "      <th>Energy_Vibes_Epic_Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.20200</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>-5.745</td>\n",
       "      <td>0.0307</td>\n",
       "      <td>92.960</td>\n",
       "      <td>0.907</td>\n",
       "      <td>1.118930e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>4.846395</td>\n",
       "      <td>98</td>\n",
       "      <td>0.307210</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>0.362314</td>\n",
       "      <td>0.385711</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.246992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.03930</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0923</td>\n",
       "      <td>-8.926</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>99.935</td>\n",
       "      <td>0.495</td>\n",
       "      <td>2.205607e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>5.327024</td>\n",
       "      <td>428</td>\n",
       "      <td>0.450053</td>\n",
       "      <td>0.192193</td>\n",
       "      <td>0.030384</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.015850</td>\n",
       "      <td>0.760386</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.000396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.54200</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3330</td>\n",
       "      <td>-6.246</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>134.001</td>\n",
       "      <td>0.275</td>\n",
       "      <td>8.756409e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>5.063918</td>\n",
       "      <td>141</td>\n",
       "      <td>0.290722</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>0.418962</td>\n",
       "      <td>0.230767</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.347208</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>0.000766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00364</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0451</td>\n",
       "      <td>-7.328</td>\n",
       "      <td>0.0616</td>\n",
       "      <td>100.904</td>\n",
       "      <td>0.796</td>\n",
       "      <td>1.049947e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>4.838269</td>\n",
       "      <td>132</td>\n",
       "      <td>0.300683</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.994590</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.17500</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1810</td>\n",
       "      <td>-5.559</td>\n",
       "      <td>0.0451</td>\n",
       "      <td>186.752</td>\n",
       "      <td>0.709</td>\n",
       "      <td>2.109060e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>5.375000</td>\n",
       "      <td>117</td>\n",
       "      <td>0.365625</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.324886</td>\n",
       "      <td>0.046997</td>\n",
       "      <td>0.624069</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>0.001014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   acousticness  danceability  energy  instrumentalness  liveness  loudness  \\\n",
       "0       0.20200         0.759   0.699          0.000131    0.4430    -5.745   \n",
       "1       0.03930         0.535   0.505          0.000000    0.0923    -8.926   \n",
       "2       0.54200         0.698   0.533          0.000000    0.3330    -6.246   \n",
       "3       0.00364         0.767   0.551          0.000000    0.0451    -7.328   \n",
       "4       0.17500         0.398   0.804          0.000000    0.1810    -5.559   \n",
       "\n",
       "   speechiness    tempo  valence    view_count  ...  avg_word_len  \\\n",
       "0       0.0307   92.960    0.907  1.118930e+09  ...      4.846395   \n",
       "1       0.2450   99.935    0.495  2.205607e+08  ...      5.327024   \n",
       "2       0.0437  134.001    0.275  8.756409e+07  ...      5.063918   \n",
       "3       0.0616  100.904    0.796  1.049947e+07  ...      4.838269   \n",
       "4       0.0451  186.752    0.709  2.109060e+07  ...      5.375000   \n",
       "\n",
       "   unique_words  vocab_richness  Rap_Street_Slang_Topic  \\\n",
       "0            98        0.307210                0.001245   \n",
       "1           428        0.450053                0.192193   \n",
       "2           141        0.290722                0.000765   \n",
       "3           132        0.300683                0.000900   \n",
       "4           117        0.365625                0.001012   \n",
       "\n",
       "   Romance_Relationships_Topic  Life_Nostalgia_Topic  \\\n",
       "0                     0.001247              0.001247   \n",
       "1                     0.030384              0.000396   \n",
       "2                     0.418962              0.230767   \n",
       "3                     0.000902              0.000901   \n",
       "4                     0.324886              0.046997   \n",
       "\n",
       "   Party_Dance_Sensuality_Topic  Love_Emotion_Sentiment_Topic  \\\n",
       "0                      0.362314                      0.385711   \n",
       "1                      0.015850                      0.760386   \n",
       "2                      0.000766                      0.347208   \n",
       "3                      0.000906                      0.994590   \n",
       "4                      0.624069                      0.001012   \n",
       "\n",
       "   Loss_Struggle_Reflection_Topic  Energy_Vibes_Epic_Topic  \n",
       "0                        0.001244                 0.246992  \n",
       "1                        0.000396                 0.000396  \n",
       "2                        0.000765                 0.000766  \n",
       "3                        0.000900                 0.000901  \n",
       "4                        0.001011                 0.001014  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in model-ready dataset CSV\n",
    "df = pd.read_csv(\"model_ready_dataset.csv\")\n",
    "print(df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad00eb8e-fd02-4403-9132-92fd8d83544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features\n",
    "X = df.drop(columns=['view_count'])\n",
    "\n",
    "# numeric columns only\n",
    "num_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "X_numeric = X[num_cols]\n",
    "\n",
    "# scale numeric columns\n",
    "X = scaler.fit_transform(X_numeric)\n",
    "\n",
    "# target\n",
    "y = df['view_count']\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e02562ff-776e-43df-9277-d4e0ee8b603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter grid\n",
    "grid_param = {\n",
    "    'n_estimators': [300, 400, 500, 600],\n",
    "    'learning_rate': [0.05, 0.01, 0.02, 0.03],\n",
    "    'max_depth': [7, 8, 9],\n",
    "    'min_child_weight': [5, 6, 7]\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=0,\n",
    "    tree_method='hist'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d715ca-3947-4a39-aee8-5767362be832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set grid search\n",
    "gd_sr = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=grid_param,\n",
    "    scoring='r2',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# fit grid search\n",
    "gd_sr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848aee9f-5e17-44f3-993e-a61a9a8d85ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best parameters / score\n",
    "best_parameters = gd_sr.best_params_\n",
    "print(\"Best Parameters:\", best_parameters)\n",
    "\n",
    "best_result = gd_sr.best_score_\n",
    "print(\"Best CV R2 Score:\", best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88e2285e-5fde-412d-a9a8-d7187b2be953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max error:  6607248574.0\n",
      "mean absolute error:  197891597.1410593\n",
      "mean squared error:  1.6487625295467702e+17\n",
      "r2 score:  0.14459614065577664\n"
     ]
    }
   ],
   "source": [
    "# fit best model\n",
    "regressor = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=1234,\n",
    "    tree_method='hist',\n",
    "    n_estimators = 300,\n",
    "    learning_rate = 0.01,\n",
    "    max_depth = 7,\n",
    "    min_child_weight = 7\n",
    ")\n",
    "\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data and compute error metrics\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "print(\"max error: \", max_error(y_test, y_pred))\n",
    "print(\"mean absolute error: \", mean_absolute_error(y_test, y_pred))\n",
    "print(\"mean squared error: \", mean_squared_error(y_test, y_pred))\n",
    "print(\"r2 score: \", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f3a7fa-2c1b-459f-a627-79704c8160b0",
   "metadata": {},
   "source": [
    "## Binary Response (Viral vs. Not Viral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dffe532-e425-4dd0-9c32-53ca702db209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create binary viral response variable\n",
    "df['viral'] = (df['view_count'] > 100_000_000).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2e7cdb-9b79-4a48-84e6-e058ed8d4c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features\n",
    "X = df.drop(columns=['viral','view_count'])\n",
    "\n",
    "# numeric columns only\n",
    "num_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "X_numeric = X[num_cols]\n",
    "\n",
    "# scale numeric columns\n",
    "X = scaler.fit_transform(X_numeric)\n",
    "\n",
    "# target\n",
    "y = df['viral']\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e27889-8436-4bed-b31b-58ce11a380b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter grid\n",
    "grid_param = {\n",
    "    'n_estimators': [300, 350, 400],\n",
    "    'learning_rate': [0.005, 0.01, 0.015],\n",
    "    'max_depth': [7, 8],\n",
    "    'min_child_weight': [6, 7]\n",
    "}\n",
    "\n",
    "xgb_clf = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    tree_method='hist',\n",
    "    random_state=0,\n",
    "    eval_metric='logloss'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe90dcc-1efa-4cd5-94e4-acce69872e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set grid search\n",
    "gd_sr = GridSearchCV(\n",
    "    estimator=xgb_clf,\n",
    "    param_grid=grid_param,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# fit grid search\n",
    "gd_sr.fit(X_train, y_train)\n",
    "\n",
    "# best parameters / score\n",
    "print(\"Best Parameters:\", gd_sr.best_params_)\n",
    "print(\"Best CV ROC AUC:\", gd_sr.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ff2cea-3b0f-4fa4-913f-83d653fd881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit best model\n",
    "model = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    tree_method='hist',\n",
    "    random_state=0,\n",
    "    n_estimators = 400,\n",
    "    learning_rate = 0.015,\n",
    "    max_depth = 8,\n",
    "    min_child_weight = 7\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647309ec-690c-463c-9d60-db2e9fbd4dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted probabilities\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# predicted class\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
