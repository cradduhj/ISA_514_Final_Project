---
title: 'Part 5 Scoring'
author: "Harrison Cradduck, Nick Berry"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
    df_print: paged
    number_sections: yes
    theme: readable
    toc: yes
    toc_float: yes
    code_download: yes
  word_document:
    toc: no
---

```{r setup, include = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
knitr::opts_chunk$set(fig.width = 8, fig.height = 6)
set.seed(123)

if(require(pacman) == 0)
   {install.packages("pacman")}
pacman::p_load(devtools, caret, cluster, dplyr, fastDummies, leaps, pacman, tidyverse, skimr, fastDummies, GGally, DataExplorer, ggrepel, ggthemes, dslabs, lubridate, stringr, tidytext, purrr, ROSE, doParallel, recipes, stringdist)

if (!require(mlba)) {
  library(devtools)
  install_github("gedeck/mlba/mlba", force = TRUE)
}
pacman::p_load(mlba, tidyverse)
```

# Introduction

This Markdown file showcases the steps taken to score our dataset by applying the same data cleaning / feature engineering techniques to the scoring data set and then choosing a few best models to score the response `loan_default` on the scoring data set and submit the predictions to the Kaggle competition.

# Data Loading

```{r}
# read in .csv file and convert all strings to factors
df <- read.csv("../data/score.csv", stringsAsFactors = TRUE)
head(df)
```

# Exploratory Data Analysis

The goal of this phase is to explore the dataset, understand its structure, detect any patterns or anomalies, and clean the data to prepare it for model building. Proper EDA and data preprocessing are essential for enhancing the quality of the dataset and ensuring the models can learn effectively from the data.

## Data Overview

### Summary Statistics

```{r}
# skim the data
skimr::skim(df)
```

### Missing Data

```{r}
# view missing values
DataExplorer::plot_missing(df)
```

```{r}
# view blank values
colSums(df == "", na.rm = TRUE)
```

**In addition to checking the missing values, we noticed that there are also blank values that may need to be dealt with. Specifically, `emp_title`, `emp_length`, `title`, and `last_credit_pull_d` have blank values that may have meaning, or not.**

## Data Cleaning

This section includes removing duplicate data, manually setting base levels for factor variables, filling in blank values, combining levels of factor variables, and removing variables from the data all together.

```{r}
# remove duplicates from data
df <- df %>% distinct()
```

**After exploring the data, we found multiple rows that are duplicated in the data. Likely, these are partners, spouses, or family members included on the same loan application or on a separate, identical loan application. We chose to keep the unique rows in the data and remove all duplicates.**

```{r}
# set "INDIVIDUAL" as the base level for `application_type`
# df$application_type <- fct_relevel(df$application_type, "INDIVIDUAL")
# 
# levels(df$application_type)
# 
# #collapse the low population levels for `application_type`
# df$application_type <- forcats::fct_collapse(df$application_type, 
#                                              Other = c('DIRECT_PAY', 'JOINT'))
# table(df$application_type)

# after looking at default rate between the two levels... there is no significant difference. That, coupled with the low population in 'the 'Other', justifies our removal of `application_type` as it is basically constant
df <- df %>% 
  dplyr::select(-application_type)
```

**REMOVE APPLICATION TYPE**

```{r}
# collapse `purpose` into the top 10 levels in terms of frequency ("other" includes all less frequent levels)
df$purpose <- forcats::fct_collapse(df$purpose, 
                                    OTHER = c("renewable_energy", "educational", "wedding", "house", "other", "medical", "moving", "small_business", "vacation", "car"))

# set the level with the highest frequency as the base level for `purpose`
df$purpose <- forcats::fct_relevel(df$purpose, names(which.max(table(df$purpose)))) # set base to most frequent level

table(df$purpose)
```

**We chose to relevel `purpose` so that the highest frequency level ("debt_consolidation") is set as the base level.**

**COLLAPSE PURPOSE FURTHER**

```{r}
# classify blanks as "unreported" in `emp_length`
df$emp_length <- forcats::fct_recode(df$emp_length, Unreported = "")

table(df$emp_length)
```

**In addition to the year values of `emp_length`, there are 11037 blank values. We chose to replace these blank values with "unreported" as an indicator of emptiness.**

```{r}
# set "10+ years" as the base level for `emp_length`
df$emp_length <- fct_relevel(df$emp_length, "10+ years")

levels(df$emp_length)
```

**We chose to relevel `emp_length` so that the highest frequency level, "10+ years", is set as the base level.**

```{r}
# classify "NONE" as "OTHER" in `home_ownership`
df$home_ownership <- forcats::fct_collapse(df$home_ownership, OTHER = c("OTHER", "NONE", "ANY"))

table(df$home_ownership)

# remove observations of "OTHER" factor level since it is so sparsely populated 
# df <- droplevels(df[!df$home_ownership == "OTHER",])

# Replace "OTHER" with "MORTGAGE" in the factor column
df$home_ownership <- as.character(df$home_ownership)  # convert to character
df$home_ownership[df$home_ownership == "OTHER"] <- "MORTGAGE"  # replace
df$home_ownership <- factor(df$home_ownership)  # convert back to factor
```

**The "NONE" and "OTHER" categories of `home_ownership` are extremely sparse compared to `MORTGAGE`, `RENT`, and `OWN. We chose to collapse "OTHER" and "NONE" into the "OTHER" category to reduce the number of levels and mitigate data sparsity.**

**REMOVE "OTHER" LEVEL OBSERVATIONS**

```{r}
# remove `hardship_flag`
df <- df %>% dplyr::select(-hardship_flag)
```

**We chose to remove `hardship_flag` from the data since it is a constant column across all observations, so it contains no predictive power.**

```{r}
# remove `installment`
df <- df %>% dplyr::select(-installment)
```

**We chose to remove `installment` from the data since it is nearly perfectly collinear with `loan_amnt`, `loan_amnt` seems more fundamental than `installment`, and `installment` could most likely be calculated using `loan_amnt`, `term`, and `int_rate`.**

```{r}
# remove `title`
df <- df %>% dplyr::select(-title)
```

**We chose to remove `title` from the data since it seems to be a user-entered, more granular (31659 levels > 14 levels), and often repetitive version of `purpose`.**

## Data Wrangling

This section includes recategorizing date variables into meaningful time variables, collapsing factor variables, binning numeric variables, numeric to factor conversions, and feature extraction.

```{r}
# recategorizing `issue_d` into years since
# df$issue_d_date <- my(df$issue_d)
# df$issue_d_years_since <- year(Sys.Date()) - year(df$issue_d_date) # difference between current year and year of issue date

# recategorizing `issue_d` into months of the year
# df$issue_d_months <- substr(df$issue_d, 0, 3) # pull out 3-letter abbreviation of month (e.g. "Apr")

# table(df$issue_d_months)

# months since
df$issue_d_date <- as.Date(paste("01", df$issue_d, sep = "-"), format = "%d-%b-%Y")
df$issue_d_months_since <- floor(as.numeric(difftime(Sys.Date(), df$issue_d_date, units = "days")) / 30.44)

# remove `issue_d` and `issue_d_date`
df <- df %>% dplyr::select(-issue_d, -issue_d_date)
```

**`issue_d` has 115 different combinations of months and years spanning from June 2007 to December 2016. To reduce this variable to something more interpretable, we chose to convert the issue date to a "years since" idea, reducing the total levels and assuming the month of loan issue has little to no significance relative to year. Depending on the performance of `issue_d_years_since` in the modeling phase, we may choose to backtrack and convert the issue date to just the month of the issue date instead, reducing the total levels to 12 and assuming the year of the loan issue has little to no significance relative to month. This stems from the idea that lenders may need to meet a certain loan amount/quantity quota for quarters or seasons within a year, sparking a change of behavior when approving loans.**

**KEPT MONTHS SINCE FOR FINAL SCORING DATA**

```{r}
# log transforming `open_acc`
# df$open_acc_log <- log1p(df$open_acc)

# capping `open_acc`
# quantile(df$open_acc, probs = seq(0.9, 1, 0.001)) # print quantiles to visually identify cutoffs
# cap_value <- quantile(df$open_acc, 0.998) # assign value at given quantile
# df$open_acc_cap <- ifelse(df$open_acc > cap_value, cap_value, df$open_acc)

# combining `open_acc` into 10 semi-equal width bins
# df$open_acc_bins <- cut(
#   df$open_acc,
#   breaks = quantile(df$open_acc, probs = seq(0, 1, 0.1)),
#   include.lowest = TRUE
# )
# 
# table(df$open_acc_bins)
```

**`open_acc` is very right-skewed with a couple large outliers. We chose to bin `open_acc` into 10 relatively equal-width bins to reduce the total levels and retain interpretability. Depending on the performance of `open_acc_bins` in the modeling phase, we may choose to backtrack and either perform a log1p (accounting for values of "0") transformation on `open_acc` to make the distribution close to normal and more symmetric, or cap `open_acc` to the 99.8th percentile to limit the influence of the large outliers on future analysis.**

**REMOVE BINNING OF OPEN_ACC**

```{r}
# recategorizing `earliest_cr_line` into years since
# df$earliest_cr_line_date <- my(df$earliest_cr_line)
# df$earliest_cr_line_years_since <- year(Sys.Date()) - year(df$earliest_cr_line_date) # difference between current year and year of earliest credit line

# combining `earliest_cr_line_years_since` into 10 semi-equal width bins
# df$earliest_cr_line_bins <- cut(
#   df$earliest_cr_line_years_since,
#   breaks = quantile(df$earliest_cr_line_years_since, probs = seq(0, 1, 0.1)),
#   include.lowest = TRUE
# )
# 
# table(df$earliest_cr_line_bins)

# months since
df$earliest_cr_line_date <- as.Date(paste("01", df$earliest_cr_line, sep = "-"), format = "%d-%b-%Y")
df$earliest_cr_line_months_since <- floor(as.numeric(difftime(Sys.Date(), df$earliest_cr_line_date, units = "days")) / 30.44)

# remove `earliest_cr_line` and `earliest_cr_line_date`
# df <- df %>% dplyr::select(-earliest_cr_line, -earliest_cr_line_date, -earliest_cr_line_years_since)
df <- df %>% dplyr::select(-earliest_cr_line, -earliest_cr_line_date)
```

**`earliest_cr_line` has 659 different combinations of months and years spanning from November 1950 to October 2013. To reduce this variable to something more interpretable, we chose to convert the issue date to a "years since" idea, reducing the total levels and assuming the month of earliest credit line has little to no significance relative to year. Additionally, we chose to bin these "years since" counts into 10 relatively equal-width bins to again reduce the total levels and retain interpretability.**

**REMOVE BINNING, KEEP MONTHS SINCE**

```{r}
# convert `address` from factor to character
df$address <- as.character(df$address)

# pull out `zip_code` from `address`
# df$zip_code <- factor(substr(df$address, nchar(df$address) - 4, nchar(df$address))) # zip code is located in the last 5 indexes

# table(df$zip_code)

# pull out `territory` from `address`
#df$territory <- factor(substr(df$address, nchar(df$address) - 7, nchar(df$address) - 6)) # territory is located within the indexes (-7,-6)

# remove `address`
df <- df %>% dplyr::select(-address)
```

**`address` is made up of 99.58% unique values. In order to treat `address` as a factor with a reasonable number of levels, we chose to pull out the territories from each address and use `territory` as a substitute for location. In addition to the 50 U.S. states, "DC" (District of Columbia), "AA" (Armed Forces America), "AE" (Armed Forces Europe, Middle East, Africa, Canada), and "AP" (Armed Forces Pacific) are included as territories in the data. We also considered pulling out zip code as a more granular location variable, but there seems to be a data entry error as only 10 unique zip codes are found in the data despite there being 54 territories.**

**REMOVE TERRITORY AND ADDRESS**

```{r}
# # recode `delinq_2yrs` into 4 levels: "0", "1", "2", "3+"
# df$delinq_2yrs <- forcats::fct_collapse(
#   df$delinq_2yrs %>% as.character(), 
#   "0" = "0",
#   "1+" = as.character(1:max(df$delinq_2yrs)))
#   #"2" = "2",
#   #"3+" = as.character(3:max(df$delinq_2yrs))) # range from 3 to the max value of the variable
# 
# # reorder levels of `delinq_2yrs`
# df$delinq_2yrs <- forcats::fct_relevel(df$delinq_2yrs, "0", "1+")
# 
# table(df$delinq_2yrs)
# levels(df$delinq_2yrs)
```

**`delinq_2yrs` is zero-dominated and thus right-skewed. To combat this, we chose to convert `delinq_2yrs` to factor and collapse the categories into 4 levels: "0", "1", "2", and "3+". We did this to reduce the number of levels, mitigate data sparsity, and classify large values of `delinq_2yrs` as equal.**

**DID NOT CONVERT TO FACTOR**

```{r}
# # recode `inq_last_6mths` into 3 levels: "0", "1", "2+"
# df$inq_last_6mths <- forcats::fct_collapse(
#   df$inq_last_6mths %>% as.character(), 
#   "0" = "0",
#   "1" = "1",
#   "2+" = as.character(2:max(df$inq_last_6mths))) # range from 2 to the max value of the variable
# 
# # reorder the levels of `inq_last_6mths`
# df$inq_last_6mths <- forcats::fct_relevel(df$inq_last_6mths, "0", "1", "2+")
# 
# table(df$inq_last_6mths)
```

**`inq_last_6mths` is zero-dominated and right-skewed. To combat this, we chose to convert `inq_last_6mths` to factor and collapse the categories into 3 levels: "0", "1", and "12". We did this to reduce the number of levels, mitigate data sparsity, and classify larger values of `inq_last_6mths` as equal.**

**DID NOT CONVERT TO FACTOR**

```{r}
# recode `acc_now_delinq` into 2 levels: "0", "1+"
# df$acc_now_delinq <- forcats::fct_collapse(
#   df$acc_now_delinq %>% as.character(), 
#   "0" = "0",
#   "1+" = as.character(1:max(as.numeric(df$acc_now_delinq))))  # range from 1 to the max value of the variable
# 
# # reorder the levels of `acc_now_delinq`
# df$acc_now_delinq <- forcats::fct_relevel(df$acc_now_delinq, "0", "1+")
# 
# table(df$acc_now_delinq)

# 2nd level is very sparse... basically a constant so I am removing the variable
df <- df %>%
  dplyr::select(-acc_now_delinq)
```

**`acc_now_delinq` is extremely zero-dominated and right-skewed. To combat this, we chose to convert `acc_now_delinq` to factor and collapse the categories into 2 levels: "0", ">0". We did this to reduce the number of levels, mitigate data sparsity, and classify larger values of `acc_now_delinq` as equal.**

**DID NOT CONVERT TO FACTOR**

**REMOVDE `acc_now_delinq`**

## Feature Engineering

This section includes slicing variables, performing log transformations, capping numeric variables, creating difference variables, creating ratio features, binning numeric variables, removing observations, imputing missing values, creating average variables, collapsing factor variables, and category mapping.

```{r}
# remove `grade`
df <- df %>% dplyr::select(-grade)

# remove lettering from `sub_grade`
# df$sub_grade_num <- as.numeric(gsub("[A-G]", "", df$sub_grade)) # substitute all letters A through G with empty space and treat resulting value as continuous

# remove `sub_grade`
# df <- df %>% dplyr::select(-sub_grade)
```

**`grade` has 7 levels with values of "A" through "G". `sub_grade` adds a number from 1 to 5 on top of these alphabetical values as a more granular risk rating within each grade. We chose to drop `grade` as the values of `grade` are included within `sub_grade`, therefore all of the predictive power within `grade` is theoretically captured by the more granular `sub_grade`. Depending on the performance of `grade` in the modeling phase, we may choose to backtrack and remove the alphabetical values from `sub_grade` as they are already included in `grade`, allowing `sub_grade_num` to be treated as continuous.**

```{r}
# log transforming `annual_inc`
#df$annual_inc_log <- log1p(df$annual_inc)

# capping `annual_inc`
# quantile(df$annual_inc, probs = seq(0.9, 1, 0.001)) # print quantiles to visually identify cutoffs
# cap_value <- quantile(df$annual_inc, 0.995) # assign value of given quantile
# df$annual_inc_cap <- ifelse(df$annual_inc > cap_value, cap_value, df$annual_inc)

# remove `annual_inc`
#df <- df %>% dplyr::select(-annual_inc)
```

**`annual_inc` is severely right-skewed with many large outliers. We chose to perform a log transformation on `annual_inc` to make the distribution close to normal and more symmetric. Depending on the performance of `annual_inc_log` in the modeling phase, we may choose to backtrack and cap `annual_inc` to the 99.5th percentile instead to limit the influence of the extreme outliers on future analysis.**

**REMOVED LOG TRANSFORMATION, KEPT ORIGINAL**

```{r}
# remove observation with value of "9999.00" for `dti`
df <- df[df$dti != 9999, ]

# log transforming `dti`
# df$dti_log <- log1p(df$dti)

# capping `dti`
# quantile(df$dti, probs = seq(0.9, 1, 0.001)) # print quantiles to visually identify cutoffs
#cap_value <- quantile(df$dti, 0.999) # assign value of given quantile
#df$dti_cap <- ifelse(df$dti > cap_value, cap_value, df$dti)

# remove `dti`
#df <- df %>% dplyr::select(-dti)
```

**`dti` is severely right-skewed with a couple large outliers. We chose to remove observation number 96957 with a value of "9999.00" because this value corresponds to infinity due to a reported value of 0 for `annaul_inc`. Additionally, we chose to cap `dti` to the 99.9th percentile to limit the influence of the extreme outliers on future analysis. Depending on the performance of `dti_cap` in the modeling phase, we may choose to backtrack and perform a log1p (accounting for values of "0") transformation on `dti` to make the distribution close to normal and more symmetric.**

**REMOVE CAP, DROPPED OUTLIER OBSERVATION**

```{r}
# log transforming `revol_bal`
#df$revol_bal_log <- log1p(df$revol_bal)

# capping `revol_bal`
# quantile(df$revol_bal, probs = seq(0.9, 1, 0.001)) # print quantiles to visually identify cutoffs
# cap_value <- quantile(df$revol_bal, 0.996) # assign value of given quantile
# df$revol_bal_cap <- ifelse(df$revol_bal > cap_value, cap_value, df$revol_bal)

# remove `revol_bal`
#df <- df %>% dplyr::select(-revol_bal)
```

**`revol_bal` is severely right-skewed with many large outliers. We chose to perform a log transformation on `revol_bal` to make the distribution close to normal and more symmetric. Depending on the performance of `revol_bal_log` in the modeling phase, we may choose to backtrack and cap `revol_bal` to the 99.6th percentile instead to limit the influence of the extreme outliers on future analysis.**

**REMOVED LOG TRANSFORMATION**

```{r}
# impute missing values of `pub_rec_bankruptcies` as 0
df$pub_rec_bankruptcies <- ifelse(is.na(df$pub_rec_bankruptcies), 0, df$pub_rec_bankruptcies)

# create `pub_rec_non_bankruptcies` as difference between `pub_rec` and `pub_rec_bankruptcies`
# df$pub_rec_non_bankruptcies <- df$pub_rec - df$pub_rec_bankruptcies
# 
# # recode `pub_rec_non_bankruptcies` into 2 levels: "0", "1+"
# df$pub_rec_non_bankruptcies <- forcats::fct_collapse(
#   df$pub_rec_non_bankruptcies %>% as.character(), 
#   "0" = "0",
#   "1+"= as.character(1:max(df$pub_rec_non_bankruptcies))) # range from 1 to the max value of the variable
# 
# table(df$pub_rec_non_bankruptcies)
# 
# # recode `pub_rec_bankruptcies` into 3 levels: "0", "1", "2+"
# df$pub_rec_bankruptcies <- forcats::fct_collapse(
#   df$pub_rec_bankruptcies %>% as.character(), 
#   "0" = "0",
#   "1+" = as.character(1:max(df$pub_rec_bankruptcies)))
  #"2+" = as.character(2:max(df$pub_rec_bankruptcies))) # range from 2 to the max value of the variable

# remove `pub_rec`
df <- df %>% dplyr::select(-pub_rec)

table(df$pub_rec_bankruptcies)
```

**There are 336 missing values in `pub_rec_bankruptcies`. We chose to impute these missing values with zeroes as the majority of the missing values in `pub_rec_bankruptcies` corresponded to a value of 0 for `pub_rec`, and `pub_rec` by nature includes `pub_rec_bankruptcies`. We then chose to create `pub_rec_non_bankruptcies` as the difference between `pub_rec` and `pub_rec_bankruptcies` to replace `pub_rec` and accommodate for double-counting bankruptcies and any multicollinearity between `pub_rec` and `pub_rec_bankruptcies`. `pub_rec_non_bankruptcies` is zero-dominated and thus very right-skewed. To combat this, we chose to convert `pub_rec_non_bankruptcies` to factor and collapse the categories into 2 levels: "0" and "1+". Additionally, `pub_rec_bankruptcies` is zero-dominated and thus very right-skewed. To combat this, we chose to convert `pub_rec_bankruptcies` to factor and collapse the categories into 3 levels: "0", "1", and "2+". We did this to reduce the number of levels and mitigate data sparsity.**

**COLLAPSED PUB_REC_BANKRUPTCIES FURTHER**

**DID NOT CONVERT TO FACTOR AND DID NOT CREATE NON-BAKCRUPTCIES VARIABLE**

```{r}
# create `closed_acc` as difference between `total_acc` and `open_acc`
df$closed_acc <- df$total_acc - df$open_acc

# log transforming `closed_acc`
# df$closed_acc_log <- log1p(df$closed_acc)

# capping `closed_acc`
# quantile(df$closed_acc, probs = seq(0.9, 1, 0.001)) # print quantiles to visually identify cutoffs
# cap_value <- quantile(df$closed_acc, 0.998) # assign value of given quantile
# df$closed_acc_cap <- ifelse(df$closed_acc > cap_value, cap_value, df$closed_acc)

# combining `closed_acc` into 10 semi-equal width bins
# df$closed_acc_bins <- cut(
#   df$closed_acc,
#   breaks = quantile(df$closed_acc, probs = seq(0, 1, 0.1)),
#   include.lowest = TRUE
# )

# remove `open_acc`, `total_acc`, and `closed_acc`
df <- df %>% dplyr::select( -total_acc)

#table(df$closed_acc_bins)
```

**We then chose to create `closed_acc` as the difference between `pub_rec` and `total_acc` to replace `open_acc` and accommodate for double-counting open accounts and any multicollinearity between `total_acc` and `open_acc`. `total_acc` is zero-dominated and thus very right-skewed. `closed_acc` is very right-skewed with a couple large outliers. We chose to bin `closed_acc` into 10 relatively equal-width bins to reduce the total levels and retain interpretability. Depending on the performance of `open_acc_bins` in the modeling phase, we may choose to backtrack and either perform a log1p (accounting for values of "0") transformation on `open_acc` to make the distribution close to normal and more symmetric, or cap `open_acc` to the 99.8th percentile to limit the influence of the large outliers on future analysis.**

**REMOVED BINNING**

```{r}
# impute missing values of `revol_util` as 0
df$revol_util <- ifelse(is.na(df$revol_util), 0, df$revol_util)

# capping `revol_util`
# quantile(df$revol_util, probs = seq(0.9, 1, 0.001)) # print quantiles to visually identify cutoffs
#cap_value <- quantile(df$revol_util, 0.999) # assign value of given quantile
#df$revol_util_cap <- ifelse(df$revol_util > cap_value, cap_value, df$revol_util)

# remove `revol_util`
#df <- df %>% dplyr::select(-revol_util)
```

**There are 177 missing values in `revol_util`. We chose to impute these missing values with zeroes as the majority of the missing values in `revol_util` corresponded to a value of 0 for `revol_bal`. Likewise, the majority of the values of 0 in `revol_util` corresponded to a value of 0 for `revol_bal` as well, so it made logical sense to treat the missing values as zeroes. `revol_util` is also very right-skewed with a couple large outliers. We chose to cap `revol_util` to the 99.9th percentile to limit the influence of the large outliers on future analysis.**

**REMOVED CAP**

```{r}
# calculate average of FICO score range
df$fico_range_avg <- (df$fico_range_high + df$fico_range_low) / 2

# remove `fico_range_low` and `fico_range_high`
df <- df %>% dplyr::select(-fico_range_low, -fico_range_high)
```

**Each observation has a value for `fico_range_low` and `fico_range_high` with a difference of either 4 or 5. To reduce the two perfectly correlated variables into one, we chose to take the average of the range.**

```{r}
# recode `mths_since_last_delinq` as factor
df$mths_since_last_delinq <- factor(df$mths_since_last_delinq)

# impute missing values of `mths_since_last_delinq as "Never"
df$mths_since_last_delinq <- ifelse(is.na(df$mths_since_last_delinq), "Never", df$mths_since_last_delinq)

# recode `mths_since_last_delinq` to year ranges from character-converted months
df$yrs_since_last_delinq <- df$mths_since_last_delinq %>%
  as.character() %>%
  fct_collapse(
    "Never" = "Never",
    "<1yr" = as.character(0:11),
    "1-2yr" = as.character(12:23),
    "2-3yr" = as.character(24:35),
    "3-4yr" = as.character(36:47),
    "4-5yr" = as.character(48:59),
    "5-6yr" = as.character(60:71),
    ">6yr" = as.character(72:max(as.numeric(df$mths_since_last_delinq[df$mths_since_last_delinq != "Never"])))) # range from 72 to the max value of the variable

# reorder the levels of `yrs_since_last_delinq`
df$yrs_since_last_delinq <- forcats::fct_relevel(df$yrs_since_last_delinq, "Never", "<1yr", "1-2yr", "2-3yr", "3-4yr", "4-5yr", "5-6yr", ">6yr")

# remove `mths_since_last_delinq`
df <- df %>% dplyr::select(-mths_since_last_delinq)

table(df$yrs_since_last_delinq)
```

**There are 126,474 missing values in `mths_since_last_delinq`. We chose to impute these missing values with "Never" as the missing values in `mths_since_last_delinq` correspond to a loan applicant never having a delinquency. Additionally, we chose to recode `mths_since_last_delinq` into year ranges to reduce the number of levels and mitigate data sparsity; grouping all values greater than 6 years into one level was done as an attempt to create semi-equally-weighted levels and resolve data sparsity.**

```{r}
# recategorizing `last_credit_pull_d` into years since
# df$last_credit_pull_d_date <- my(df$last_credit_pull_d)
# df$last_credit_pull_d_years_since <- year(Sys.Date()) - year(df$last_credit_pull_d_date) # difference between current year and year of issue date

# impute blank values of `last_credit_pull_d_years_since as "Never"
# df$last_credit_pull_d_years_since <- factor(ifelse(is.na(df$last_credit_pull_d_years_since), "Never", df$last_credit_pull_d_years_since))

# regroup `last_credit_pull_d_years_since` to year ranges
# df$last_credit_pull_d_years_since <- df$last_credit_pull_d_years_since %>%
  # as.character() %>%
  # fct_collapse(
    # "Never" = "Never",
    # "6" = "6",
    # "7" = "7",
    # "8" = "8",
    # "9" = "9",
    # "10+" = as.character(10:18)) # range from 10 to 18

# reorder the levels of `last_credit_pull_d_years_since`
# df$last_credit_pull_d_years_since <- forcats::fct_relevel(df$last_credit_pull_d_years_since, "Never", "6", "7", "8", "9", "10+")

df$last_credit_pull_d_date <- as.Date(paste("01", df$last_credit_pull_d, sep = "-"), format = "%d-%b-%Y")
df$last_credit_pull_d_months_since <- floor(as.numeric(difftime(Sys.Date(), df$last_credit_pull_d_date, units = "days")) / 30.44)

# remove `last_credit_pull_d` and `last_credit_pull_d_date`
df <- df %>% dplyr::select(-last_credit_pull_d, -last_credit_pull_d_date)
# remove missing values
# df <- df[!is.na(df$last_credit_pull_d_months_since),]
df$last_credit_pull_d_months_since[is.na(df$last_credit_pull_d_months_since)] <- 94

table(df$last_credit_pull_d_months_since)
summary(df$last_credit_pull_d_months_since)
```

**In addition to the date values of `last_credit_pull_d`, there are 12 blank values. We chose to replace these blank values with "Never" as an indicator of a loan applicant never having their credit pulled. `last_credit_pull_d` has 137 different combinations of months and years spanning from June 2007 to March 2019. To reduce this variable to something more interpretable, we chose to convert the issue date to a "years since" idea, reducing the total levels and assuming the month of last credit pull has little to no significance relative to year. Additionally, we chose to recode `last_credit_pull_d_years_since` into year ranges to again reduce the number of levels and mitigate data sparsity.**

**KEEP AS NUMERIC YEAR VALUE INSTEAD OF FACTOR AND REMOVE MISSING, CONVERT TO MONTHS SINCE**

```{r}
# clean `emp_title`
df$emp_title_clean <- df$emp_title %>%
  as.character() %>% # convert to character
  str_trim() %>% # remove all trailing or leading spaces
  str_to_lower() # convert all letters to lowercase

# count the alphabetical, non-modifier words in `emp_title_clean`
word_counts <- df %>%
  select(emp_title_clean) %>%
  unnest_tokens(word, emp_title_clean) %>% # split into words
  filter(!word %in% stop_words$word, # remove common prepositions, conjunctions, and modifiers
         str_detect(word, "[a-z]")) %>% # keep only letters
  count(word, sort = TRUE)

# grab the 200 most common words
common_words <- head(word_counts, 200)

# group `emp_title` into broad categories based on the most common words by pattern matching using regex()
df$emp_title_group <- case_when(
  str_detect(df$emp_title_clean, regex("teach|school|professor|instructor|principal|university|education|college", ignore_case = TRUE)) ~ "Education",
  str_detect(df$emp_title_clean, regex("nurse|rn|clinical|therapist|physician|healthcare|dental|pharmacy|technologist|nursing", ignore_case = TRUE)) ~ "Healthcare",
  str_detect(df$emp_title_clean, regex("driver|truck|mechanic|transportation|bus|logistics|carrier", ignore_case = TRUE)) ~ "Transportation",
  str_detect(df$emp_title_clean, regex("manager|president|vp|vice|chief|director|supervisor|leader|executive|superintendent", ignore_case = TRUE)) ~ "Management",
  str_detect(df$emp_title_clean, regex("account|accountant|accounting|accounts|banker|finance|financial|controller|auditor|credit|loan|insurance|mortgage|underwriter|claims|tax", ignore_case = TRUE)) ~ "Business",
  str_detect(df$emp_title_clean, regex("sales|sale|rep|representative|customer|client|advisor|agent|marketing|store|associate", ignore_case = TRUE)) ~ "Sales",
  str_detect(df$emp_title_clean, regex("clerk|coordinator|secretary|administrative|admin|office|support|assistant|asst", ignore_case = TRUE)) ~ "Administration",
  str_detect(df$emp_title_clean, regex("engineer|engineering|software|programmer|developer|technician|technical|tech|technology|network|data|it|system|computer|design|designer", ignore_case = TRUE)) ~ "Tech",
  str_detect(df$emp_title_clean, regex("police|sheriff|fire|sergeant|correctional|army|military|navy|federal|public|postal|government", ignore_case = TRUE)) ~ "Public/Government",
  str_detect(df$emp_title_clean, regex("construction|electrician|foreman|mechanic|maintenance|equipment|operator|machin|labor|inspector|mech", ignore_case = TRUE)) ~ "Trades",
  str_detect(df$emp_title_clean, regex("attorney|paralegal|legal|compliance", ignore_case = TRUE)) ~ "Legal",
  str_detect(df$emp_title_clean, regex("owner|self|llc|corp|corporation|company|associates|llp", ignore_case = TRUE)) ~ "Self-Employed",
  TRUE ~ "Other"
)

# remove `emp_title` and `emp_title_clean`
df <- df %>% dplyr::select(-emp_title, -emp_title_clean)

table(df$emp_title_group)
```

**`emp_title` is another user-entered variable with 112,646 different unique values. We chose to combine values containing the most common words/strings into a much smaller set of broad categories to hopefully gain predictive power. Disclaimer: ChatGPT was leveraged to help with the syntax for counting words and for the tediousness of mapping words to categories.**

```{r}
# set the level with the highest frequency as the base level for `emp_title_group`
df$emp_title_group <- forcats::fct_relevel(df$emp_title_group, names(which.max(table(df$emp_title_group))))

levels(df$emp_title_group)
```

**We chose to relevel `emp_title_group` so that the highest frequency level ("Other") is set as the base level.**

```{r}
# recode `term` to years for consistency, and remove leading spaces
df$term <- as.factor(ifelse(df$term == " 36 months", "3 Years", "5 Years"))

table(df$term)
```

**We chose to rename the values of `term` from months to years to be consistent with our general feature engineering approach for time variables. Additionally, there was a leading space in front of the original values, so we removed it.**

```{r}
# create missing value indicator of `mort_acc`
df$mort_acc_NA <- ifelse(is.na(df$mort_acc), 1, 0)

# impute missing values of `mort_acc` as median of `home_ownership` groupings
# need to impute values from training data 
train <- readRDS('median impute training.rds')

medians_train <- train %>%
  group_by(home_ownership) %>%
  summarise(median_mort_acc = median(mort_acc, na.rm = TRUE)) %>%
  ungroup()

df <- df %>%
  left_join(medians_train, by = "home_ownership") %>%  # Joining the median values
  mutate(mort_acc = ifelse(is.na(mort_acc), median_mort_acc, mort_acc)) %>%  # Impute using the precomputed median
  select(-median_mort_acc)

# recode `mort_acc` into 5 levels
df$mort_acc <- forcats::fct_collapse(
  df$mort_acc %>% as.character(), 
  "0" = "0",
  "1" = "1",
  "2" = "2",
  "3-4" = as.character(3:4),
  "5+" = as.character(5:max(df$mort_acc))) # range from 72 to the max value of the variable

# reorder levels of `mort_acc`
df$mort_acc <- forcats::fct_relevel(df$mort_acc, "0", "1", "2", "3-4", "5+")

table(df$mort_acc)
```

**There are 22854 missing values in `mort_acc`. We chose to impute these missing values with the median of the given level of `home_ownership` for each corresponding observation. Since `mort_acc` seems to logically be related to `home_ownership` in some fashion, we chose to impute using `home_ownership` rather than just simple imputation. `mort_acc` is zero-dominated and thus very right-skewed. To combat this, we chose to convert `mort_acc` to factor and collapse the categories into 5 levels: "0", "1", "2", "3-4", and "5+". We did this to avoid issues with skew, mitigate data sparsity, and classify larger values of `mort_acc` as equal.**

# Model Loading

Loading in the RDS files with the best models for scoring. These best models were chosen based on evaluation metrics such as F1, Youden's J, AUC, accuracy, specificity, and sensitivity from Parts 2 through 4 of the Loan Default project.

```{r}
# read in RDS files
xgb_model1 <- readRDS("raw_xgboost.rds")
xgb_model2 <- readRDS("raw_XGBoost_dmy.rds")
rf_model1 <- readRDS("raw_random_forest.rds")
rf_model2 <- readRDS("raw_random_forest_dmy.rds")
```

```{r}
# creating the dummies on the training data 
dummies <- caret::dummyVars(~ ., 
                            data = df,
                            fullRank = FALSE)

# making the dataframe for the dummies and reintroducing the observed class
df_dummy <- data.frame(predict(dummies, newdata = df))
```

## Model Scoring

Score `loan_default` on the score dataset, create predictions dataframes, and save to csv for Kaggle submission.

```{r}
# predict using the trained model
y_pred <- predict(xgb_model1, newdata = df)

y_pred <- ifelse(as.character(predict(xgb_model1, newdata = df)) == "Yes", 1, 0)

# combine predictions with IDs
predictions <- data.frame(ID = df$ID, loan_status = y_pred)

head(predictions)

# write to CSV
write.csv(predictions, "group9A_Berry-Cradduck_xgb1_submission_file.csv", row.names = FALSE)
```

```{r}
# predict using the trained model
y_pred <- predict(xgb_model2, newdata = df_dummy)

y_pred <- ifelse(as.character(predict(xgb_model2, newdata = df_dummy)) == "Yes", 1, 0)

# combine predictions with IDs
predictions <- data.frame(ID = df$ID, loan_status = y_pred)

head(predictions)

# write to CSV
write.csv(predictions, "group9A_Berry-Cradduck_xgb2_submission_file.csv", row.names = FALSE)
```

```{r}
# predict using the trained model
y_pred <- predict(rf_model1, newdata = df)

y_pred <- ifelse(as.character(predict(rf_model1, newdata = df)) == "Yes", 1, 0)

# combine predictions with IDs
predictions <- data.frame(ID = df$ID, loan_status = y_pred)

head(predictions)

# write to CSV
write.csv(predictions, "group9A_Berry-Cradduck_rf1_submission_file.csv", row.names = FALSE)
```

```{r}
# predict using the trained model
y_pred <- predict(rf_model2, newdata = df_dummy)

y_pred <- ifelse(as.character(predict(rf_model2, newdata = df_dummy)) == "Yes", 1, 0)

# combine predictions with IDs
predictions <- data.frame(ID = df$ID, loan_status = y_pred)

head(predictions)

# write to CSV
write.csv(predictions, "group9A_Berry-Cradduck_rf2_submission_file.csv", row.names = FALSE)
```
